{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sonnet_generate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbWXe0ir1x6d"
      },
      "source": [
        "#Importing required libraries \n",
        "import numpy as np \n",
        "import tensorflow as tf"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Hf5418y15pA"
      },
      "source": [
        "# Initializing Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "#Reading Data\n",
        "data = open(r'/content/drive/My Drive/dataset.txt').read()\n",
        "\n",
        "#Making sentences Lower case and spliting it \n",
        "corpus = data.lower().split(\"<eos>\")\n",
        "corpus = corpus[:100]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA94NtaJ18SF"
      },
      "source": [
        "#Cleaning data, removing unwanted symbols.\n",
        "import re\n",
        "for i in range(len(corpus)):\n",
        " corpus[i] = corpus[i].replace(\"\\n\",' ')\n",
        " corpus[i] = corpus[i].replace(\"<eos>\",' ')\n",
        " corpus[i] = re.sub(\"[`~!@#$+%*:().'?-].\\\"\", ' ',corpus[i]) "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOVpx7OA1-pd"
      },
      "source": [
        "#Fitting data on tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow.keras.utils as ku \n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1 \n",
        "\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "        \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]                                                                               \n",
        "label = ku.to_categorical(label, num_classes=total_words)        "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmgcQXnc2EzZ"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import Regularizer\n",
        "#Sequential Model \n",
        "model = Sequential()\n",
        "\n",
        "#Adding layers to the model \n",
        "model.add(Embedding(total_words,240, input_length=max_sequence_len-1))  \n",
        "model.add(Bidirectional(LSTM(150))) \n",
        "model.add(Dropout(0.2)) \n",
        "model.add(Dense(total_words/2, activation='relu'))\n",
        "model.add(Dense(total_words, activation='softmax'))  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQP1ADuQ2GQf"
      },
      "source": [
        "#Compiling Model \n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUZx02E72IOZ",
        "outputId": "96e3b8d7-9c0b-4a0a-8811-ce1cc8336fa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Checkpoint \n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy') > 0.82):\n",
        "      model.save('model.h5')\n",
        "      print(\"\\nReached 80% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()\n",
        "#Training model over certain epoches .\n",
        "model.fit(predictors, label, epochs=1,callbacks = [callbacks], verbose=1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22/22 [==============================] - 0s 8ms/step - loss: 6.0954 - accuracy: 0.0185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f867f18a780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AsyZd2ofVIp"
      },
      "source": [
        "#to read the .json and HDF5 file from the drive\n",
        "from tensorflow.keras.models import model_from_json\n",
        "json_file = open('/content/drive/My Drive/jack1.json','r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "loaded_model.load_weights('/content/drive/My Drive/model.h5')\n",
        "model = loaded_model\n",
        "#Feeding the trained model to predict the words\n",
        "def predict(seed_text , seed = 625):\n",
        "    for i in range( seed ):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=\n",
        "        max_sequence_len , padding='pre')\n",
        "        predicted = model.predict_classes(token_list, verbose=0 )\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "        if i!=1 and i!=0:\n",
        "          if i%13==0:\n",
        "            seed_text+='\\n'\n",
        "    return seed_text\n",
        "\n",
        "\n",
        "#Giving seed text for generation of the sonnet\n",
        "#data = open(r'/content/drive/My Drive/dataset2.txt').read()\n",
        "sonnet_14 = input()\n",
        "tk=predict(sonnet_14)\n",
        "d=tk.split(\"\\n\")[1:15]\n",
        "for i in range(len(d)):  \n",
        "  d[i] = re.sub(\"[,`~!@#$+%*:()'?-]\", '',d[i])\n",
        "h1=d.pop(0).split(\" \")\n",
        "if(h1[0].lower()=='d' or h1[0].lower()=='s'or h1[0].lower()=='Ã¨'):h1[0]=\"As\"\n",
        "for i in([\" \".join(h1).capitalize()]+d):\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}